{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERT LoRA Fine-tuning for Meter Classification\n",
    "\n",
    "**GPU**: Runtime → Change runtime type → T4 GPU (free) or A100 (Pro)\n",
    "\n",
    "This notebook:\n",
    "1. Downloads METER2800 dataset from Harvard Dataverse (local, fast)\n",
    "2. Downloads WIKIMETER from YouTube to **Google Drive** (persistent, slow)\n",
    "3. Fine-tunes MERT with LoRA for multi-label meter classification\n",
    "4. Saves checkpoints to Drive every epoch (crash-safe)\n",
    "\n",
    "**Resumable**: Just re-run all cells. Everything skips what's already done."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Configuration\n\nChange these before running. Everything else auto-configures.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ┌──────────────────────────────────────────┐\n",
    "# │  CHANGE THESE BEFORE RUNNING             │\n",
    "# └──────────────────────────────────────────┘\n",
    "\n",
    "MODEL_NAME = \"m-a-p/MERT-v1-330M\"  # or \"m-a-p/MERT-v1-95M\" for free T4\n",
    "RUN_NAME = None    # None = auto-unique (params + timestamp + random suffix); set manually to resume same run.\n",
    "AUTO_RESUME_TRACKED_RUN = True  # when RUN_NAME is None, reuse last tracked run name from Drive\n",
    "USE_LORA = True\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "EPOCHS = 80\n",
    "PATIENCE = 15      # early stopping patience (epochs without improvement)\n",
    "NOISE_STD = 0.01   # augmentation noise\n",
    "HEAD_DROPOUT = 0.4\n",
    "WIKI_VAL_RATIO = 0.1  # fraction of WIKIMETER songs held out for val\n",
    "NUM_WORKERS = 2     # DataLoader workers in Colab (2 is a safe default)\n",
    "CHUNK_BATCH_SIZE = 0  # 0=auto (2 on 16-24GB GPUs, 4 on >=30GB); higher is faster but uses more VRAM\n",
    "AUDIO_CACHE_DIR = \"/content/audio_cache_24k\"  # set to None to disable waveform cache\n",
    "USE_AMP = True      # mixed precision on CUDA\n",
    "AMP_DTYPE = \"bf16\"  # \"bf16\" (L4/A100) or \"fp16\" (fallback / older GPUs)\n",
    "USE_TF32 = True     # faster matmul/conv on NVIDIA Ampere+\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install & GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft librosa scikit-learn tqdm yt-dlp\n",
    "!apt-get -qq install ffmpeg\n",
    "\n",
    "# Install torchaudio only if missing, matching installed torch build.\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if importlib.util.find_spec(\"torchaudio\") is None:\n",
    "    import torch\n",
    "    torch_ver = torch.__version__.split(\"+\")[0]\n",
    "    cuda_ver = (torch.version.cuda or \"\").replace(\".\", \"\")\n",
    "    index_url = f\"https://download.pytorch.org/whl/cu{cuda_ver}\" if cuda_ver else \"https://download.pytorch.org/whl/cpu\"\n",
    "    subprocess.check_call([\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"-q\",\n",
    "        \"--index-url\",\n",
    "        index_url,\n",
    "        f\"torchaudio=={torch_ver}\",\n",
    "    ])\n",
    "\n",
    "import torchaudio\n",
    "print(f\"torchaudio: {torchaudio.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Helps reduce CUDA allocator fragmentation on long Colab runs.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'NO GPU'}\")\n",
    "print(f\"CUDA alloc conf: {os.environ.get('PYTORCH_CUDA_ALLOC_CONF', '')}\")\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"No GPU! Go to Runtime → Change runtime type → T4 GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive & setup paths\n",
    "\n",
    "All persistent data lives on Drive:\n",
    "- `wikimeter/audio/` — YouTube segments (survive restarts)\n",
    "- `checkpoints/` — model checkpoints (every epoch)\n",
    "- `wikimeter.json` — song catalog (upload once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Persistent paths (Google Drive)\n",
    "DRIVE_DIR = Path(\"/content/drive/MyDrive/beatmeter\")\n",
    "DRIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WIKIMETER_DIR = DRIVE_DIR / \"wikimeter\"\n",
    "WIKIMETER_AUDIO = WIKIMETER_DIR / \"audio\"\n",
    "WIKIMETER_TAB = WIKIMETER_DIR / \"data_wikimeter.tab\"\n",
    "CATALOG_PATH = DRIVE_DIR / \"wikimeter.json\"\n",
    "CHECKPOINT_DIR = DRIVE_DIR / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Local paths (ephemeral, re-downloaded each session)\n",
    "METER2800_DIR = Path(\"/content/data/meter2800\")\n",
    "METER2800_AUDIO = METER2800_DIR / \"audio\"\n",
    "\n",
    "print(f\"Drive dir:     {DRIVE_DIR}\")\n",
    "print(f\"Wikimeter:     {WIKIMETER_DIR}\")\n",
    "print(f\"Checkpoints:   {CHECKPOINT_DIR}\")\n",
    "print(f\"METER2800:     {METER2800_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload wikimeter.json (first time only)\n",
    "\n",
    "Upload `scripts/setup/wikimeter.json` from the repo. Only needed once — it stays on Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if CATALOG_PATH.exists():\n",
    "    catalog = json.loads(CATALOG_PATH.read_text())\n",
    "    print(f\"Catalog already on Drive: {len(catalog)} songs\")\n",
    "else:\n",
    "    from google.colab import files\n",
    "    print(\"Upload wikimeter.json from scripts/setup/wikimeter.json\")\n",
    "    uploaded = files.upload()\n",
    "    for fname, data in uploaded.items():\n",
    "        CATALOG_PATH.write_bytes(data)\n",
    "        catalog = json.loads(data)\n",
    "        print(f\"Saved {len(catalog)} songs to {CATALOG_PATH}\")\n",
    "\n",
    "# Show balance\n",
    "from collections import Counter\n",
    "cats = Counter()\n",
    "for e in catalog:\n",
    "    keys = list(e['meters'].keys())\n",
    "    if len(keys) == 1:\n",
    "        cats[keys[0]] += 1\n",
    "    else:\n",
    "        cats['poly'] += 1\n",
    "print(f\"Balance: {dict(sorted(cats.items()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download METER2800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import ssl\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "AUDIO_EXTENSIONS = {\".wav\", \".mp3\", \".ogg\", \".flac\", \".oga\", \".opus\", \".aiff\", \".aif\"}\n",
    "DOI = \"doi:10.7910/DVN/0CLXBQ\"\n",
    "API_BASE = \"https://dataverse.harvard.edu/api\"\n",
    "TAR_FILES = {\"FMA.tar.gz\", \"MAG.tar.gz\", \"OWN.tar.gz\"}\n",
    "\n",
    "\n",
    "def api_request(url, max_retries=3):\n",
    "    ctx = ssl.create_default_context()\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers={\"User-Agent\": \"RhythmAnalyzer/1.0\"})\n",
    "            with urllib.request.urlopen(req, timeout=30, context=ctx) as resp:\n",
    "                return resp.read()\n",
    "        except Exception:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 * (2 ** attempt))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "def download_dataverse_file(file_id, dest):\n",
    "    url = f\"{API_BASE}/access/datafile/{file_id}\"\n",
    "    ctx = ssl.create_default_context()\n",
    "    req = urllib.request.Request(url, headers={\"User-Agent\": \"RhythmAnalyzer/1.0\"})\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with urllib.request.urlopen(req, timeout=300, context=ctx) as resp:\n",
    "        dest.write_bytes(resp.read())\n",
    "\n",
    "\n",
    "def extract_tar(tar_path, audio_dir):\n",
    "    audio_dir.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "            name = Path(member.name).name\n",
    "            if Path(name).suffix.lower() not in AUDIO_EXTENSIONS:\n",
    "                continue\n",
    "            dest = audio_dir / name\n",
    "            if dest.exists():\n",
    "                dest = audio_dir / f\"{Path(member.name).parent.name}_{name}\"\n",
    "            with tar.extractfile(member) as src:\n",
    "                if src:\n",
    "                    dest.write_bytes(src.read())\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "if METER2800_AUDIO.exists() and len(list(METER2800_AUDIO.glob(\"*\"))) > 2000:\n",
    "    n = len([f for f in METER2800_AUDIO.iterdir() if f.suffix.lower() in AUDIO_EXTENSIONS])\n",
    "    print(f\"METER2800 already downloaded: {n} audio files\")\n",
    "else:\n",
    "    METER2800_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    downloads_dir = METER2800_DIR / \"downloads\"\n",
    "    downloads_dir.mkdir(parents=True, exist_ok=True)\n",
    "    METER2800_AUDIO.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"Fetching METER2800 metadata...\")\n",
    "    metadata = json.loads(api_request(f\"{API_BASE}/datasets/:persistentId/?persistentId={DOI}\"))\n",
    "    files = metadata[\"data\"][\"latestVersion\"][\"files\"]\n",
    "\n",
    "    for f in files:\n",
    "        df = f[\"dataFile\"]\n",
    "        fname = df[\"filename\"]\n",
    "        if fname in TAR_FILES:\n",
    "            dest = downloads_dir / fname\n",
    "            if not dest.exists():\n",
    "                print(f\"  Downloading {fname}...\", end=\" \", flush=True)\n",
    "                download_dataverse_file(df[\"id\"], dest)\n",
    "                print(\"OK\")\n",
    "        elif fname.endswith((\".csv\", \".tab\", \".tsv\")):\n",
    "            dest = METER2800_DIR / fname\n",
    "            if not dest.exists():\n",
    "                print(f\"  Downloading {fname}...\", end=\" \", flush=True)\n",
    "                download_dataverse_file(df[\"id\"], dest)\n",
    "                print(\"OK\")\n",
    "\n",
    "    total = 0\n",
    "    for tar_path in sorted(downloads_dir.glob(\"*.tar.gz\")):\n",
    "        print(f\"  Extracting {tar_path.name}...\", end=\" \", flush=True)\n",
    "        count = extract_tar(tar_path, METER2800_AUDIO)\n",
    "        print(f\"{count} files\")\n",
    "        total += count\n",
    "    print(f\"Total: {total} audio files\")\n",
    "\n",
    "n_audio = len([f for f in METER2800_AUDIO.iterdir() if f.suffix.lower() in AUDIO_EXTENSIONS])\n",
    "n_tabs = len(list(METER2800_DIR.glob(\"*.tab\")))\n",
    "print(f\"METER2800: {n_audio} audio, {n_tabs} label files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download WIKIMETER (YouTube → Google Drive)\n",
    "\n",
    "Downloads are stored on Drive — survives Colab restarts.\n",
    "Re-running skips already downloaded songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "import unicodedata\n",
    "\n",
    "METER_LABELS = {3: \"three\", 4: \"four\", 5: \"five\", 7: \"seven\", 9: \"nine\", 11: \"eleven\"}\n",
    "MAX_SEGMENTS = 5\n",
    "SEGMENT_LENGTH = 35\n",
    "\n",
    "\n",
    "def _slugify(name, ascii_only=False):\n",
    "    s = name.lower()\n",
    "    if ascii_only:\n",
    "        s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[^\\w\\s-]\", \"\", s)\n",
    "    s = re.sub(r\"[\\s]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s[:80]\n",
    "\n",
    "\n",
    "def sanitize_filename(artist, title):\n",
    "    # Canonical stem for new downloads (stable ASCII form).\n",
    "    return _slugify(f\"{artist}_{title}\", ascii_only=True)\n",
    "\n",
    "\n",
    "def legacy_sanitize_filename(artist, title):\n",
    "    # Backward-compatible stem used by older notebook versions.\n",
    "    return _slugify(f\"{artist}_{title}\", ascii_only=False)\n",
    "\n",
    "\n",
    "def get_duration(path):\n",
    "    try:\n",
    "        r = subprocess.run(\n",
    "            [\"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\", \"-show_format\", str(path)],\n",
    "            capture_output=True, text=True, timeout=10)\n",
    "        return float(json.loads(r.stdout)[\"format\"][\"duration\"])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_and_segment(video_id, stem, audio_dir):\n",
    "    \"\"\"Download by video_id, segment, return list of segment stems.\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmp_path = Path(tmpdir) / \"full.%(ext)s\"\n",
    "        url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "        cmd = [\"yt-dlp\", url, \"-x\", \"--audio-format\", \"mp3\", \"--audio-quality\", \"5\",\n",
    "               \"-o\", str(tmp_path), \"--no-playlist\", \"--quiet\", \"--no-warnings\"]\n",
    "        try:\n",
    "            r = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "            if r.returncode != 0:\n",
    "                return []\n",
    "        except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "            return []\n",
    "\n",
    "        downloaded = list(Path(tmpdir).glob(\"full.*\"))\n",
    "        if not downloaded:\n",
    "            return []\n",
    "        src = downloaded[0]\n",
    "\n",
    "        duration = get_duration(src)\n",
    "        if not duration or duration < 15:\n",
    "            return []\n",
    "\n",
    "        # Segment evenly across track\n",
    "        margin = 10.0 if duration > 40 else 0.0\n",
    "        usable_start = margin\n",
    "        usable_duration = duration - 2 * margin\n",
    "        if usable_duration < 15:\n",
    "            usable_start, usable_duration = 0, duration\n",
    "\n",
    "        MIN_GAP = 5.0\n",
    "        stride = SEGMENT_LENGTH + MIN_GAP\n",
    "        n_seg = min(MAX_SEGMENTS, max(1, int((usable_duration + MIN_GAP) // stride)))\n",
    "\n",
    "        if n_seg == 1:\n",
    "            starts = [max(0.0, (usable_duration - SEGMENT_LENGTH) / 2)]\n",
    "        else:\n",
    "            actual_stride = (usable_duration - SEGMENT_LENGTH) / (n_seg - 1)\n",
    "            starts = [i * actual_stride for i in range(n_seg)]\n",
    "\n",
    "        audio_dir.mkdir(parents=True, exist_ok=True)\n",
    "        segments = []\n",
    "        for idx, offset in enumerate(starts):\n",
    "            seg_dur = min(SEGMENT_LENGTH, usable_duration - offset)\n",
    "            if seg_dur < 15:\n",
    "                continue\n",
    "            seg_stem = f\"{stem}_seg{idx:02d}\"\n",
    "            dest = audio_dir / f\"{seg_stem}.mp3\"\n",
    "            try:\n",
    "                r = subprocess.run(\n",
    "                    [\"ffmpeg\", \"-y\", \"-i\", str(src),\n",
    "                     \"-ss\", f\"{usable_start + offset:.1f}\", \"-t\", f\"{seg_dur:.1f}\",\n",
    "                     \"-acodec\", \"libmp3lame\", \"-ab\", \"192k\", \"-ar\", \"44100\", \"-ac\", \"1\",\n",
    "                     str(dest)],\n",
    "                    capture_output=True, text=True, timeout=60)\n",
    "                if r.returncode == 0 and dest.exists() and dest.stat().st_size > 1000:\n",
    "                    segments.append(seg_stem)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                pass\n",
    "        return segments\n",
    "\n",
    "\n",
    "def meters_to_tab_str(meters_dict):\n",
    "    parts = []\n",
    "    for m, w in meters_dict.items():\n",
    "        parts.append(str(m) if w == 1.0 else f\"{m}:{w}\")\n",
    "    return \",\".join(parts)\n",
    "\n",
    "\n",
    "# Load catalog\n",
    "catalog = json.loads(CATALOG_PATH.read_text())\n",
    "print(f\"WIKIMETER: {len(catalog)} songs\")\n",
    "\n",
    "# Download (skips existing)\n",
    "successful = []\n",
    "stats = {\"ok\": 0, \"skip\": 0, \"fail\": 0}\n",
    "\n",
    "for i, song in enumerate(catalog, 1):\n",
    "    artist, title = song[\"artist\"], song[\"title\"]\n",
    "    video_id = song[\"video_id\"]\n",
    "    meters = {int(k): v for k, v in song[\"meters\"].items()}\n",
    "    stem = sanitize_filename(artist, title)\n",
    "    legacy_stem = legacy_sanitize_filename(artist, title)\n",
    "    meters_str = \"+\".join(str(m) for m in meters)\n",
    "\n",
    "    existing = []\n",
    "    if WIKIMETER_AUDIO.exists():\n",
    "        existing_map = {}\n",
    "        for stem_candidate in {stem, legacy_stem}:\n",
    "            for f in WIKIMETER_AUDIO.glob(f\"{stem_candidate}_seg*.mp3\"):\n",
    "                if f.stat().st_size > 1000:\n",
    "                    existing_map[str(f)] = f\n",
    "        existing = sorted(existing_map.values(), key=lambda x: x.name)\n",
    "    if existing:\n",
    "        for f in existing[:MAX_SEGMENTS]:\n",
    "            successful.append((f.stem, meters))\n",
    "        stats[\"skip\"] += 1\n",
    "        continue\n",
    "\n",
    "    print(f\"  [{i:3d}/{len(catalog)}] {artist} — {title} ({meters_str})\", end=\"\", flush=True)\n",
    "    segments = download_and_segment(video_id, stem, WIKIMETER_AUDIO)\n",
    "\n",
    "    if segments:\n",
    "        print(f\" — {len(segments)} segs\")\n",
    "        for s in segments:\n",
    "            successful.append((s, meters))\n",
    "        stats[\"ok\"] += 1\n",
    "    else:\n",
    "        print(\" — FAILED\")\n",
    "        stats[\"fail\"] += 1\n",
    "\n",
    "# Write .tab\n",
    "WIKIMETER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(WIKIMETER_TAB, \"w\") as f:\n",
    "    f.write(\"filename\\tlabel\\tmeter\\talt_meter\\n\")\n",
    "    for stem, meters in successful:\n",
    "        meter_list = list(meters.keys())\n",
    "        primary = meter_list[0]\n",
    "        label = METER_LABELS.get(primary, str(primary))\n",
    "        meter_str = meters_to_tab_str(meters)\n",
    "        f.write(f'\"/WIKIMETER/{stem}.mp3\"\\t\"{label}\"\\t{meter_str}\\t{primary * 2}\\n')\n",
    "\n",
    "# Summary\n",
    "seg_cats = Counter()\n",
    "for _, meters in successful:\n",
    "    for m in meters:\n",
    "        seg_cats[m] += 1\n",
    "seg_summary = \", \".join(f\"{seg_cats[m]}×{m}/x\" for m in sorted(seg_cats))\n",
    "print(f\"\\nDone: {stats['ok']} downloaded, {stats['skip']} skipped, {stats['fail']} failed\")\n",
    "print(f\"Segments: {len(successful)} ({seg_summary})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4b. Copy WIKIMETER to local SSD (faster I/O)\n\nDrive FUSE is slow. Copying ~900MB to `/content/` local SSD speeds up training significantly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "WIKIMETER_LOCAL = Path(\"/content/wikimeter_local\")\n",
    "WIKIMETER_LOCAL_AUDIO = WIKIMETER_LOCAL / \"audio\"\n",
    "WIKIMETER_LOCAL_TAB = WIKIMETER_LOCAL / \"data_wikimeter.tab\"\n",
    "\n",
    "if WIKIMETER_AUDIO.exists():\n",
    "    n_drive = len(list(WIKIMETER_AUDIO.glob(\"*.mp3\")))\n",
    "    n_local = len(list(WIKIMETER_LOCAL_AUDIO.glob(\"*.mp3\"))) if WIKIMETER_LOCAL_AUDIO.exists() else 0\n",
    "\n",
    "    if n_local >= n_drive and n_local > 0:\n",
    "        print(f\"WIKIMETER already on local SSD: {n_local} files\")\n",
    "    else:\n",
    "        print(f\"Copying WIKIMETER to local SSD ({n_drive} files)...\", end=\" \", flush=True)\n",
    "        if WIKIMETER_LOCAL.exists():\n",
    "            shutil.rmtree(WIKIMETER_LOCAL)\n",
    "        WIKIMETER_LOCAL.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copytree(WIKIMETER_AUDIO, WIKIMETER_LOCAL_AUDIO)\n",
    "        if WIKIMETER_TAB.exists():\n",
    "            shutil.copy2(WIKIMETER_TAB, WIKIMETER_LOCAL_TAB)\n",
    "        n_copied = len(list(WIKIMETER_LOCAL_AUDIO.glob(\"*.mp3\")))\n",
    "        print(f\"Done — {n_copied} files\")\n",
    "else:\n",
    "    WIKIMETER_LOCAL = None\n",
    "    print(\"No WIKIMETER audio on Drive yet\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    HAS_TORCHAUDIO = True\n",
    "except Exception:\n",
    "    torchaudio = None\n",
    "    HAS_TORCHAUDIO = False\n",
    "\n",
    "if not HAS_TORCHAUDIO:\n",
    "    print(\"WARNING: torchaudio unavailable; falling back to librosa decoding\")\n",
    "\n",
    "CLASS_METERS = [3, 4, 5, 7, 9, 11]\n",
    "METER_TO_IDX = {m: i for i, m in enumerate(CLASS_METERS)}\n",
    "IDX_TO_METER = {i: m for i, m in enumerate(CLASS_METERS)}\n",
    "MERT_SR = 24000\n",
    "CHUNK_SAMPLES = 5 * MERT_SR\n",
    "MAX_DURATION_S = 30\n",
    "LABEL_SMOOTH_NEG = 0.1\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"m-a-p/MERT-v1-95M\": {\"num_layers\": 12, \"hidden_dim\": 768,\n",
    "                           \"head_lr\": 1e-3, \"lora_lr\": 5e-5, \"batch_size\": 8, \"grad_accum\": 4},\n",
    "    \"m-a-p/MERT-v1-330M\": {\"num_layers\": 24, \"hidden_dim\": 1024,\n",
    "                            \"head_lr\": 5e-4, \"lora_lr\": 5e-5, \"batch_size\": 2, \"grad_accum\": 16},\n",
    "}\n",
    "\n",
    "\n",
    "def resolve_audio_path(raw_fname, data_dir):\n",
    "    raw_fname = raw_fname.strip('\"').strip(\"'\").strip()\n",
    "    if not raw_fname:\n",
    "        return None\n",
    "    p = Path(raw_fname)\n",
    "    src_dir = p.parent.name\n",
    "    stem = p.stem\n",
    "    audio_dir = data_dir / \"audio\"\n",
    "    for ext in (\".mp3\", \".wav\", \".ogg\", \".flac\", p.suffix):\n",
    "        for candidate in [\n",
    "            audio_dir / f\"{stem}{ext}\",\n",
    "            audio_dir / f\"{src_dir}_{stem}{ext}\",\n",
    "            audio_dir / f\"{src_dir}_._{stem}{ext}\",\n",
    "        ]:\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_label_file(label_path, data_dir, valid_meters=None):\n",
    "    filename_cols = [\"filename\", \"file\", \"audio_file\", \"audio_path\", \"audio\", \"path\"]\n",
    "    label_cols = [\"meter\", \"time_signature\", \"ts\", \"time_sig\", \"signature\", \"label\"]\n",
    "    entries = []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "    delimiter = \"\\t\" if \"\\t\" in first_line else \",\"\n",
    "    with open(label_path, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=delimiter)\n",
    "        if not reader.fieldnames:\n",
    "            return entries\n",
    "        header_map = {h.strip().lower().strip('\"'): h for h in reader.fieldnames}\n",
    "        fname_key = next((header_map[c] for c in filename_cols if c in header_map), None)\n",
    "        label_key = next((header_map[c] for c in label_cols if c in header_map), None)\n",
    "        if not fname_key or not label_key:\n",
    "            return entries\n",
    "        for row in reader:\n",
    "            raw_fname = row.get(fname_key, \"\").strip().strip('\"')\n",
    "            raw_label = row.get(label_key, \"\").strip().strip('\"')\n",
    "            if not raw_fname or not raw_label:\n",
    "                continue\n",
    "            try:\n",
    "                meter = int(raw_label.split(\"/\")[0].strip())\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if valid_meters and meter not in valid_meters:\n",
    "                continue\n",
    "            audio_path = resolve_audio_path(raw_fname, data_dir)\n",
    "            if audio_path:\n",
    "                entries.append((audio_path, meter))\n",
    "    return entries\n",
    "\n",
    "\n",
    "def load_split(data_dir, split):\n",
    "    valid = set(METER_TO_IDX.keys())\n",
    "    for ext in (\".tab\", \".csv\", \".tsv\"):\n",
    "        p = data_dir / f\"data_{split}_4_classes{ext}\"\n",
    "        if p.exists():\n",
    "            raw = parse_label_file(p, data_dir, valid)\n",
    "            entries = [(path, [m]) for path, m in raw]\n",
    "            print(f\"  {split}: {len(entries)} entries\")\n",
    "            return entries\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_delimiter(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_line = f.readline()\n",
    "    return \"\\t\" if \"\\t\" in first_line else \",\"\n",
    "\n",
    "\n",
    "def load_extra_data(extra_dir):\n",
    "    extra_dir = Path(extra_dir)\n",
    "    valid = set(METER_TO_IDX.keys())\n",
    "    entries = []\n",
    "    for tab_file in sorted(p for p in extra_dir.iterdir() if p.suffix in (\".tab\", \".csv\", \".tsv\")):\n",
    "        delimiter = detect_delimiter(tab_file)\n",
    "        with open(tab_file, newline=\"\") as fh:\n",
    "            reader = csv.DictReader(fh, delimiter=delimiter)\n",
    "            for row in reader:\n",
    "                raw_fname = row.get(\"filename\", \"\").strip().strip('\"')\n",
    "                raw_meter = row.get(\"meter\", \"\").strip().strip('\"')\n",
    "                if not raw_fname or not raw_meter:\n",
    "                    continue\n",
    "                # Parse soft labels: \"3:0.9,4:0.8\" or hard: \"3,4\" or \"3\"\n",
    "                meters = []\n",
    "                for part in raw_meter.split(\",\"):\n",
    "                    part = part.strip()\n",
    "                    if \":\" in part:\n",
    "                        m = int(part.split(\":\")[0])\n",
    "                    else:\n",
    "                        m = int(part)\n",
    "                    if m in valid:\n",
    "                        meters.append(m)\n",
    "                if not meters:\n",
    "                    continue\n",
    "                audio_path = resolve_audio_path(raw_fname, extra_dir)\n",
    "                if audio_path:\n",
    "                    entries.append((audio_path, meters))\n",
    "    return entries\n",
    "\n",
    "\n",
    "def _audio_cache_key(path):\n",
    "    st = path.stat()\n",
    "    raw = f\"{path.resolve()}::{st.st_size}::{st.st_mtime_ns}::{MERT_SR}\"\n",
    "    return hashlib.sha1(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def _decode_audio_resampled(path):\n",
    "    if HAS_TORCHAUDIO:\n",
    "        try:\n",
    "            wave, sr = torchaudio.load(str(path))\n",
    "            if wave.ndim == 2 and wave.shape[0] > 1:\n",
    "                wave = wave.mean(dim=0, keepdim=True)\n",
    "            if sr != MERT_SR:\n",
    "                wave = torchaudio.functional.resample(wave, sr, MERT_SR)\n",
    "            return wave.squeeze(0).cpu().numpy().astype(np.float32, copy=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        audio, _ = librosa.load(str(path), sr=MERT_SR, mono=True)\n",
    "        return audio.astype(np.float32, copy=False)\n",
    "    except Exception:\n",
    "        return np.zeros(MERT_SR, dtype=np.float32)\n",
    "\n",
    "\n",
    "class MERTAudioDataset(Dataset):\n",
    "    def __init__(self, entries, augment=False, noise_std=0.01, cache_dir=None):\n",
    "        self.entries = []\n",
    "        self.cache_paths = []\n",
    "        self.augment = augment\n",
    "        self.noise_std = noise_std\n",
    "        self.cache_dir = Path(cache_dir).resolve() if cache_dir is not None else None\n",
    "        if self.cache_dir is not None:\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for p, m in entries:\n",
    "            if p.exists() and any(x in METER_TO_IDX for x in m):\n",
    "                valid = [x for x in m if x in METER_TO_IDX]\n",
    "                if not valid:\n",
    "                    continue\n",
    "                self.entries.append((p, valid))\n",
    "                if self.cache_dir is not None:\n",
    "                    self.cache_paths.append(self.cache_dir / f\"{_audio_cache_key(p)}.npy\")\n",
    "                else:\n",
    "                    self.cache_paths.append(None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, meters = self.entries[idx]\n",
    "        cache_path = self.cache_paths[idx]\n",
    "\n",
    "        label = np.full(len(CLASS_METERS), LABEL_SMOOTH_NEG, dtype=np.float32)\n",
    "        for m in meters:\n",
    "            if m in METER_TO_IDX:\n",
    "                label[METER_TO_IDX[m]] = 1.0\n",
    "\n",
    "        audio = None\n",
    "        if cache_path is not None and cache_path.exists():\n",
    "            try:\n",
    "                audio = np.load(cache_path, allow_pickle=False).astype(np.float32, copy=False)\n",
    "            except Exception:\n",
    "                audio = None\n",
    "        if audio is None:\n",
    "            audio = _decode_audio_resampled(path)\n",
    "            if cache_path is not None:\n",
    "                tmp_name = f\"{cache_path.name}.tmp-{os.getpid()}-{time.time_ns()}\"\n",
    "                tmp_path = cache_path.with_name(tmp_name)\n",
    "                try:\n",
    "                    with open(tmp_path, \"wb\") as fh:\n",
    "                        np.save(fh, audio.astype(np.float32, copy=False))\n",
    "                    os.replace(tmp_path, cache_path)\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        tmp_path.unlink(missing_ok=True)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        max_samples = MAX_DURATION_S * MERT_SR\n",
    "        if len(audio) > max_samples:\n",
    "            start = np.random.randint(0, len(audio) - max_samples) if self.augment else (len(audio) - max_samples) // 2\n",
    "            audio = audio[start:start + max_samples]\n",
    "        if len(audio) < MERT_SR:\n",
    "            audio = np.pad(audio, (0, MERT_SR - len(audio)))\n",
    "        if self.augment:\n",
    "            audio = audio + self.noise_std * np.random.randn(len(audio)).astype(np.float32)\n",
    "            audio = np.roll(audio, np.random.randint(-MERT_SR // 2, MERT_SR // 2))\n",
    "        return audio.astype(np.float32), label\n",
    "\n",
    "\n",
    "def simple_collate(batch):\n",
    "    audios, labels = zip(*batch)\n",
    "    return list(audios), torch.tensor(np.stack(labels), dtype=torch.float32)\n",
    "\n",
    "\n",
    "class MERTClassificationHead(nn.Module):\n",
    "    def __init__(self, num_layers, pooled_dim, num_classes=6, head_dim=256, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_logits = nn.Parameter(torch.zeros(num_layers))\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(pooled_dim),\n",
    "            nn.Linear(pooled_dim, head_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, stacked):\n",
    "        w = torch.softmax(self.layer_logits, dim=0).view(1, self.num_layers, 1)\n",
    "        fused = (stacked * w).sum(dim=1)\n",
    "        return self.head(fused)\n",
    "\n",
    "\n",
    "def mert_forward_pool(audios, mert_model, processor, device, num_layers, hidden_dim, chunk_batch_size=1):\n",
    "    def _is_oom_error(exc):\n",
    "        return \"out of memory\" in str(exc).lower()\n",
    "\n",
    "    def _run_chunk_items(chunk_items, per_audio_means, per_audio_maxes, batch_size):\n",
    "        if not chunk_items:\n",
    "            return\n",
    "\n",
    "        start = 0\n",
    "        target_bs = max(1, int(batch_size))\n",
    "        current_bs = target_bs\n",
    "        warned_oom_downscale = False\n",
    "\n",
    "        while start < len(chunk_items):\n",
    "            bs = min(current_bs, len(chunk_items) - start)\n",
    "            batch_items = chunk_items[start:start + bs]\n",
    "            audio_idxs = [aidx for aidx, _ in batch_items]\n",
    "            chunk_batch = [chunk for _, chunk in batch_items]\n",
    "\n",
    "            try:\n",
    "                inputs = processor(\n",
    "                    chunk_batch,\n",
    "                    sampling_rate=MERT_SR,\n",
    "                    return_tensors=\"pt\",\n",
    "                    return_attention_mask=False,\n",
    "                    padding=True,\n",
    "                )\n",
    "                inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "                outputs = mert_model(**inputs)\n",
    "            except RuntimeError as exc:\n",
    "                if not _is_oom_error(exc) or bs == 1:\n",
    "                    raise\n",
    "                current_bs = max(1, bs // 2)\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "                if not warned_oom_downscale:\n",
    "                    print(f\"  WARNING: OOM in chunk batching; reducing chunk batch size to {current_bs}\")\n",
    "                    warned_oom_downscale = True\n",
    "                continue\n",
    "\n",
    "            for li in range(num_layers):\n",
    "                hs = outputs.hidden_states[li + 1]  # (bs, T, hidden)\n",
    "                means = hs.mean(dim=1)\n",
    "                maxes = hs.max(dim=1).values\n",
    "                for bi, audio_idx in enumerate(audio_idxs):\n",
    "                    per_audio_means[audio_idx][li].append(means[bi])\n",
    "                    per_audio_maxes[audio_idx][li].append(maxes[bi])\n",
    "\n",
    "            start += bs\n",
    "            current_bs = target_bs\n",
    "\n",
    "    per_audio_means = [[[] for _ in range(num_layers)] for _ in audios]\n",
    "    per_audio_maxes = [[[] for _ in range(num_layers)] for _ in audios]\n",
    "\n",
    "    full_chunks = []\n",
    "    tail_chunks = []\n",
    "    for audio_idx, audio_np in enumerate(audios):\n",
    "        chunks = [audio_np[s:s + CHUNK_SAMPLES] for s in range(0, len(audio_np), CHUNK_SAMPLES)\n",
    "                  if len(audio_np[s:s + CHUNK_SAMPLES]) >= MERT_SR]\n",
    "        if not chunks:\n",
    "            chunks = [audio_np]\n",
    "        for chunk in chunks:\n",
    "            if len(chunk) == CHUNK_SAMPLES:\n",
    "                full_chunks.append((audio_idx, chunk))\n",
    "            else:\n",
    "                tail_chunks.append((audio_idx, chunk))\n",
    "\n",
    "    _run_chunk_items(full_chunks, per_audio_means, per_audio_maxes, batch_size=max(1, chunk_batch_size))\n",
    "    _run_chunk_items(tail_chunks, per_audio_means, per_audio_maxes, batch_size=1)\n",
    "\n",
    "    batch_pooled = []\n",
    "    for audio_idx in range(len(audios)):\n",
    "        layer_pooled = []\n",
    "        for li in range(num_layers):\n",
    "            mean_agg = torch.stack(per_audio_means[audio_idx][li]).mean(dim=0)\n",
    "            max_agg = torch.stack(per_audio_maxes[audio_idx][li]).max(dim=0).values\n",
    "            layer_pooled.append(torch.cat([mean_agg, max_agg]))\n",
    "        batch_pooled.append(torch.stack(layer_pooled))\n",
    "\n",
    "    return torch.stack(batch_pooled)\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(mert_model, head, processor, loader, criterion, optimizer,\n",
    "                    device, num_layers, hidden_dim, grad_accum=1, use_lora=True,\n",
    "                    use_amp=False, amp_dtype=torch.float16, scaler=None, chunk_batch_size=1):\n",
    "    head.train()\n",
    "    if use_lora:\n",
    "        mert_model.train()\n",
    "    total_loss = correct = total = 0\n",
    "    amp_enabled = bool(use_amp and device.type == \"cuda\")\n",
    "    use_scaler = bool(scaler is not None and scaler.is_enabled())\n",
    "    trainable_params = list(head.parameters()) + [p for p in mert_model.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    for step, (audios, labels) in enumerate(pbar):\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=amp_enabled):\n",
    "            if use_lora:\n",
    "                pooled = mert_forward_pool(audios, mert_model, processor, device, num_layers, hidden_dim, chunk_batch_size)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    pooled = mert_forward_pool(audios, mert_model, processor, device, num_layers, hidden_dim, chunk_batch_size)\n",
    "                pooled = pooled.detach()\n",
    "            logits = head(pooled)\n",
    "            loss = criterion(logits, labels) / grad_accum\n",
    "\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        if (step + 1) % grad_accum == 0 or step == len(loader) - 1:\n",
    "            if use_scaler:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "            if use_scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss += loss.item() * grad_accum * len(audios)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels.argmax(dim=1)).sum().item()\n",
    "        total += len(audios)\n",
    "        pbar.set_postfix_str(f\"loss={total_loss/total:.3f} acc={correct/total:.0%}\")\n",
    "\n",
    "    return total_loss / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(mert_model, head, processor, loader, criterion, device, num_layers, hidden_dim,\n",
    "             use_amp=False, amp_dtype=torch.float16, chunk_batch_size=1):\n",
    "    head.eval()\n",
    "    mert_model.eval()\n",
    "    total_loss = correct = total = 0\n",
    "    all_labels_idx, all_preds_idx = [], []\n",
    "    all_probs, all_labels_raw = [], []\n",
    "    amp_enabled = bool(use_amp and device.type == \"cuda\")\n",
    "\n",
    "    for audios, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=amp_dtype, enabled=amp_enabled):\n",
    "            pooled = mert_forward_pool(audios, mert_model, processor, device, num_layers, hidden_dim, chunk_batch_size)\n",
    "            logits = head(pooled)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        total_loss += loss.item() * len(audios)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        true_primary = labels.argmax(dim=1)\n",
    "        correct += (preds == true_primary).sum().item()\n",
    "        total += len(audios)\n",
    "        all_labels_idx.extend(true_primary.cpu().tolist())\n",
    "        all_preds_idx.extend(preds.cpu().tolist())\n",
    "        all_probs.append(probs.detach().float().cpu().numpy())\n",
    "        all_labels_raw.append(labels.detach().float().cpu().numpy())\n",
    "    return (total_loss / max(total, 1), correct / max(total, 1),\n",
    "            all_labels_idx, all_preds_idx,\n",
    "            np.concatenate(all_probs), np.concatenate(all_labels_raw))\n",
    "\n",
    "\n",
    "def print_eval_metrics(labels_idx, preds_idx, probs, labels_multihot):\n",
    "    num_classes = len(CLASS_METERS)\n",
    "    labels_binary = (labels_multihot > 0.5).astype(np.float32)\n",
    "\n",
    "    # Per-class accuracy\n",
    "    print(f\"\\n{'Meter':>6s}  {'Correct':>7s}  {'Total':>5s}  {'Acc':>6s}  {'AP':>6s}\")\n",
    "    print(\"-\" * 40)\n",
    "    total_correct = total_count = 0\n",
    "    aps = []\n",
    "    for i, m in enumerate(CLASS_METERS):\n",
    "        mask = np.array(labels_idx) == i\n",
    "        n = mask.sum()\n",
    "        if n == 0:\n",
    "            print(f\"{m:>4d}/x  {'—':>7s}  {0:>5d}  {'—':>6s}  {'—':>6s}\")\n",
    "            continue\n",
    "        c = (np.array(preds_idx)[mask] == i).sum()\n",
    "        acc = c / n\n",
    "        total_correct += c\n",
    "        total_count += n\n",
    "        # AP\n",
    "        ap_str = \"—\"\n",
    "        if labels_binary[:, i].sum() > 0:\n",
    "            ap = average_precision_score(labels_binary[:, i], probs[:, i])\n",
    "            aps.append(ap)\n",
    "            ap_str = f\"{ap:.3f}\"\n",
    "        print(f\"{m:>4d}/x  {c:>5d}/{n:<5d}       {acc:>5.1%}  {ap_str:>6s}\")\n",
    "    if total_count:\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{'Total':>6s}  {total_correct:>5d}/{total_count:<5d}       {total_correct/total_count:>5.1%}  mAP={np.mean(aps):.3f}\" if aps else \"\")\n",
    "\n",
    "    # Macro-F1\n",
    "    preds_binary = (probs > 0.5).astype(np.int32)\n",
    "    cols = [i for i in range(num_classes) if labels_binary[:, i].sum() > 0]\n",
    "    if cols:\n",
    "        mf1 = f1_score(labels_binary[:, cols], preds_binary[:, cols], average='macro', zero_division=0)\n",
    "        print(f\"Macro-F1: {mf1:.3f}\")\n",
    "\n",
    "    # Confidence gap\n",
    "    sorted_p = np.sort(probs, axis=1)[:, ::-1]\n",
    "    gap = sorted_p[:, 0] - sorted_p[:, 1]\n",
    "    print(f\"Confidence gap: mean={gap.mean():.3f}, median={np.median(gap):.3f}\")\n",
    "\n",
    "\n",
    "def format_per_class_watchdog(labels_idx, preds_idx, probs, labels_multihot):\n",
    "    labels_arr = np.array(labels_idx)\n",
    "    preds_arr = np.array(preds_idx)\n",
    "    labels_binary = (labels_multihot > 0.5).astype(np.float32)\n",
    "    chunks = []\n",
    "    for i, m in enumerate(CLASS_METERS):\n",
    "        mask = labels_arr == i\n",
    "        support = int(mask.sum())\n",
    "        if support == 0:\n",
    "            chunks.append(f\"{m}/x n=0 acc=— ap=—\")\n",
    "            continue\n",
    "        c = int((preds_arr[mask] == i).sum())\n",
    "        acc = c / support\n",
    "        ap_str = \"—\"\n",
    "        if labels_binary[:, i].sum() > 0:\n",
    "            ap_str = f\"{average_precision_score(labels_binary[:, i], probs[:, i]):.3f}\"\n",
    "        chunks.append(f\"{m}/x n={support} acc={acc:.0%} ap={ap_str}\")\n",
    "    return \" | \".join(chunks)\n",
    "\n",
    "\n",
    "print(\"Training code loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Setup model & data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════\n",
    "#  AUTO-DERIVED CONFIG (from top-level settings)\n",
    "# ══════════════════════════════════════════\n",
    "\n",
    "_cfg = MODEL_CONFIGS[MODEL_NAME]\n",
    "BATCH_SIZE = _cfg[\"batch_size\"]     # 95M: 8, 330M: 4\n",
    "GRAD_ACCUM = _cfg[\"grad_accum\"]     # 95M: 4, 330M: 8 → effective batch = 32\n",
    "HEAD_LR = _cfg[\"head_lr\"]           # 95M: 1e-3, 330M: 5e-4\n",
    "LORA_LR = _cfg[\"lora_lr\"]          # 95M: 5e-5, 330M: 5e-5\n",
    "\n",
    "run_name_state_path = CHECKPOINT_DIR / \".last_run_name.txt\"\n",
    "\n",
    "if RUN_NAME is None and AUTO_RESUME_TRACKED_RUN and run_name_state_path.exists():\n",
    "    tracked = run_name_state_path.read_text(encoding=\"utf-8\").strip()\n",
    "    if tracked:\n",
    "        RUN_NAME = tracked\n",
    "        print(f\"Tracked run name: {RUN_NAME}\")\n",
    "\n",
    "if RUN_NAME is None:\n",
    "    from datetime import datetime\n",
    "    import secrets\n",
    "\n",
    "    _model_short = MODEL_NAME.split(\"/\")[-1].replace(\"MERT-v1-\", \"\")\n",
    "    _train_tag = f\"lora_r{LORA_RANK}_a{LORA_ALPHA}\" if USE_LORA else \"frozen\"\n",
    "    _hp_tag = f\"hlr{HEAD_LR:.0e}_llr{LORA_LR:.0e}_bs{BATCH_SIZE}_ga{GRAD_ACCUM}_n{NOISE_STD:g}_d{HEAD_DROPOUT:g}\"\n",
    "    _stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    _nonce = secrets.token_hex(2)\n",
    "    RUN_NAME = f\"{_model_short}_{_train_tag}_{_hp_tag}_{_stamp}_{_nonce}\".replace(\"+\", \"\")\n",
    "    print(f\"Generated run name: {RUN_NAME}\")\n",
    "\n",
    "run_name_state_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "run_name_state_path.write_text(RUN_NAME + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "CKPT_PATH = CHECKPOINT_DIR / f\"meter_mert_{RUN_NAME}.pt\"\n",
    "BEST_CKPT_PATH = CHECKPOINT_DIR / f\"meter_mert_{RUN_NAME}.best.pt\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "num_layers = _cfg[\"num_layers\"]\n",
    "hidden_dim = _cfg[\"hidden_dim\"]\n",
    "pooled_dim = hidden_dim * 2\n",
    "\n",
    "if USE_TF32 and device.type == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "_amp_mode = str(AMP_DTYPE).strip().lower()\n",
    "if _amp_mode not in {\"bf16\", \"fp16\"}:\n",
    "    raise ValueError(f\"AMP_DTYPE must be 'bf16' or 'fp16', got: {AMP_DTYPE}\")\n",
    "if _amp_mode == \"bf16\" and device.type == \"cuda\" and not torch.cuda.is_bf16_supported():\n",
    "    print(\"WARNING: bf16 not supported on this GPU; falling back to fp16\")\n",
    "    _amp_mode = \"fp16\"\n",
    "AMP_TORCH_DTYPE = torch.bfloat16 if _amp_mode == \"bf16\" else torch.float16\n",
    "AMP_ENABLED = bool(USE_AMP and device.type == \"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP_ENABLED and AMP_TORCH_DTYPE == torch.float16)\n",
    "\n",
    "if CHUNK_BATCH_SIZE > 0:\n",
    "    CHUNK_BATCH_SIZE_EFFECTIVE = int(CHUNK_BATCH_SIZE)\n",
    "elif device.type == \"cuda\":\n",
    "    _vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "    CHUNK_BATCH_SIZE_EFFECTIVE = 4 if _vram_gb >= 30 else 2\n",
    "else:\n",
    "    CHUNK_BATCH_SIZE_EFFECTIVE = 1\n",
    "\n",
    "print(f\"Config: {MODEL_NAME}\")\n",
    "print(f\"  batch={BATCH_SIZE}, grad_accum={GRAD_ACCUM} (effective={BATCH_SIZE*GRAD_ACCUM})\")\n",
    "print(f\"  head_lr={HEAD_LR}, lora_lr={LORA_LR}\")\n",
    "print(f\"  noise={NOISE_STD}, dropout={HEAD_DROPOUT}\")\n",
    "print(f\"  epochs={EPOCHS}, patience={PATIENCE}\")\n",
    "print(f\"  amp={AMP_ENABLED} ({_amp_mode}), tf32={USE_TF32 and device.type == 'cuda'}\")\n",
    "print(f\"  chunk_batch={CHUNK_BATCH_SIZE_EFFECTIVE}\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading data...\")\n",
    "train_entries = load_split(METER2800_DIR, \"train\")\n",
    "val_entries = load_split(METER2800_DIR, \"val\")\n",
    "test_entries = load_split(METER2800_DIR, \"test\")\n",
    "\n",
    "# WIKIMETER: use local SSD copy if available (faster I/O)\n",
    "_wiki_dir = WIKIMETER_LOCAL if (WIKIMETER_LOCAL is not None and WIKIMETER_LOCAL_TAB.exists()) else WIKIMETER_DIR\n",
    "_wiki_tab = _wiki_dir / \"data_wikimeter.tab\"\n",
    "if _wiki_tab.exists():\n",
    "    wiki_all = load_extra_data(_wiki_dir)\n",
    "    _src = \"local SSD\" if _wiki_dir == WIKIMETER_LOCAL else \"Drive\"\n",
    "    print(f\"  WIKIMETER: {len(wiki_all)} segments total (from {_src})\")\n",
    "\n",
    "    # Group segments by song (stem without _segXX)\n",
    "    from collections import defaultdict\n",
    "    import random\n",
    "    random.seed(42)  # reproducible split\n",
    "\n",
    "    song_segments = defaultdict(list)  # song_stem → [(path, meters), ...]\n",
    "    for path, meters in wiki_all:\n",
    "        song_stem = re.sub(r\"_seg\\d+$\", \"\", path.stem)\n",
    "        song_segments[song_stem].append((path, meters))\n",
    "\n",
    "    # Group songs by primary meter for stratified split\n",
    "    meter_songs = defaultdict(list)  # meter → [song_stem, ...]\n",
    "    for song_stem, segments in song_segments.items():\n",
    "        primary_meter = segments[0][1][0]  # first segment's first meter\n",
    "        meter_songs[primary_meter].append(song_stem)\n",
    "\n",
    "    # Pick ~10% songs per meter for val\n",
    "    wiki_val_songs = set()\n",
    "    for meter, songs in sorted(meter_songs.items()):\n",
    "        random.shuffle(songs)\n",
    "        if len(songs) <= 1:\n",
    "            n_val = 0\n",
    "        else:\n",
    "            n_val = max(1, int(len(songs) * WIKI_VAL_RATIO))\n",
    "            n_val = min(n_val, len(songs) - 1)  # keep at least one song in train\n",
    "        wiki_val_songs.update(songs[:n_val])\n",
    "\n",
    "    # Split segments\n",
    "    wiki_train, wiki_val = [], []\n",
    "    for song_stem, segments in song_segments.items():\n",
    "        if song_stem in wiki_val_songs:\n",
    "            wiki_val.extend(segments)\n",
    "        else:\n",
    "            wiki_train.extend(segments)\n",
    "\n",
    "    train_entries.extend(wiki_train)\n",
    "    val_entries.extend(wiki_val)\n",
    "    print(f\"  WIKIMETER split: {len(wiki_train)} train ({len(song_segments) - len(wiki_val_songs)} songs)\"\n",
    "          f\" + {len(wiki_val)} val ({len(wiki_val_songs)} songs)\")\n",
    "\n",
    "counts = Counter()\n",
    "for _, meters in train_entries + val_entries + test_entries:\n",
    "    for m in meters:\n",
    "        counts[m] += 1\n",
    "print(f\"\\nTotal: {len(train_entries)} train, {len(val_entries)} val, {len(test_entries)} test\")\n",
    "for m in CLASS_METERS:\n",
    "    print(f\"  {m}/x: {counts.get(m, 0)}\")\n",
    "\n",
    "# Load MERT\n",
    "print(f\"\\nLoading {MODEL_NAME}...\")\n",
    "from transformers import AutoModel, Wav2Vec2FeatureExtractor\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "mert_model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True, output_hidden_states=True).to(device)\n",
    "\n",
    "if USE_LORA:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    lora_config = LoraConfig(r=LORA_RANK, lora_alpha=LORA_ALPHA,\n",
    "                             target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "    mert_model = get_peft_model(mert_model, lora_config)\n",
    "    trainable, total_p = mert_model.get_nb_trainable_parameters()\n",
    "    print(f\"  LoRA: {trainable:,} / {total_p:,} ({trainable/total_p:.2%})\")\n",
    "else:\n",
    "    for p in mert_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "head = MERTClassificationHead(num_layers, pooled_dim, len(CLASS_METERS),\n",
    "                               dropout=HEAD_DROPOUT).to(device)\n",
    "\n",
    "# Datasets\n",
    "AUDIO_CACHE_PATH = Path(AUDIO_CACHE_DIR).expanduser().resolve() if AUDIO_CACHE_DIR else None\n",
    "if AUDIO_CACHE_PATH is not None:\n",
    "    AUDIO_CACHE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "audio_backend = \"torchaudio\" if HAS_TORCHAUDIO else \"librosa\"\n",
    "print(f\"Audio: backend={audio_backend}, cache={AUDIO_CACHE_PATH if AUDIO_CACHE_PATH is not None else 'disabled'}\")\n",
    "\n",
    "train_ds = MERTAudioDataset(train_entries, augment=True, noise_std=NOISE_STD, cache_dir=AUDIO_CACHE_PATH)\n",
    "val_ds = MERTAudioDataset(val_entries, cache_dir=AUDIO_CACHE_PATH)\n",
    "test_ds = MERTAudioDataset(test_entries, cache_dir=AUDIO_CACHE_PATH)\n",
    "\n",
    "loader_workers = max(0, int(NUM_WORKERS))\n",
    "loader_kwargs = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"collate_fn\": simple_collate,\n",
    "    \"num_workers\": loader_workers,\n",
    "    \"pin_memory\": device.type == \"cuda\",\n",
    "}\n",
    "if loader_workers > 0:\n",
    "    loader_kwargs[\"persistent_workers\"] = True\n",
    "\n",
    "train_loader = DataLoader(train_ds, shuffle=True, **loader_kwargs)\n",
    "val_loader = DataLoader(val_ds, shuffle=False, **loader_kwargs)\n",
    "test_loader = DataLoader(test_ds, shuffle=False, **loader_kwargs)\n",
    "print(f\"DataLoader: workers={loader_workers}, pin_memory={loader_kwargs['pin_memory']}, \"\n",
    "      f\"persistent_workers={loader_kwargs.get('persistent_workers', False)}\")\n",
    "\n",
    "# Loss with pos_weight\n",
    "pos_counts = np.zeros(len(CLASS_METERS), dtype=np.float32)\n",
    "for _, meters in train_ds.entries:\n",
    "    for m in meters:\n",
    "        if m in METER_TO_IDX:\n",
    "            pos_counts[METER_TO_IDX[m]] += 1\n",
    "neg_counts = len(train_ds) - pos_counts\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    pos_weights = torch.tensor(np.where(pos_counts > 0, neg_counts / pos_counts, 1.0), dtype=torch.float32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "# Optimizer\n",
    "param_groups = [{\"params\": head.parameters(), \"lr\": HEAD_LR}]\n",
    "if USE_LORA:\n",
    "    lora_params = [p for p in mert_model.parameters() if p.requires_grad]\n",
    "    if lora_params:\n",
    "        param_groups.append({\"params\": lora_params, \"lr\": LORA_LR})\n",
    "optimizer = torch.optim.AdamW(param_groups, weight_decay=1e-4)\n",
    "\n",
    "# Scheduler: ReduceLROnPlateau — drops LR only when val stops improving\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Resume from checkpoint (auto-detect)\n",
    "START_EPOCH = 1\n",
    "best_val_acc = -1.0\n",
    "if CKPT_PATH.exists():\n",
    "    print(f\"\\nResuming from {CKPT_PATH.name}\")\n",
    "    ckpt = torch.load(CKPT_PATH, weights_only=False, map_location=device)\n",
    "    head.load_state_dict(ckpt[\"head_state_dict\"])\n",
    "    if ckpt.get(\"lora_state_dict\") and USE_LORA:\n",
    "        for name, param_data in ckpt[\"lora_state_dict\"].items():\n",
    "            parts = name.split(\".\")\n",
    "            obj = mert_model\n",
    "            for part in parts[:-1]:\n",
    "                obj = getattr(obj, part)\n",
    "            getattr(obj, parts[-1]).data.copy_(param_data.to(device))\n",
    "    if \"optimizer_state_dict\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    if \"scheduler_state_dict\" in ckpt:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "    START_EPOCH = ckpt.get(\"epoch\", 0) + 1\n",
    "    best_val_acc = ckpt.get(\"val_accuracy\", -1.0)\n",
    "    print(f\"  Epoch {START_EPOCH}, best val: {best_val_acc:.1%}\")\n",
    "\n",
    "print(f\"\\nRun: {RUN_NAME}\")\n",
    "print(f\"Ready: {len(train_ds)} train, {len(val_ds)} val, {len(test_ds)} test\")\n",
    "\n",
    "print(f\"Checkpoint: {CKPT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════\n",
    "#  TRAINING LOOP\n",
    "# ══════════════════════════════════════════\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"{'Ep':>3s}  {'TrLoss':>7s}  {'TrAcc':>6s}  {'VaLoss':>7s}  {'VaAcc':>6s}  {'LR':>8s}  {'Time':>5s}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        mert_model, head, processor, train_loader, criterion, optimizer,\n",
    "        device, num_layers, hidden_dim, GRAD_ACCUM, USE_LORA,\n",
    "        AMP_ENABLED, AMP_TORCH_DTYPE, scaler, CHUNK_BATCH_SIZE_EFFECTIVE)\n",
    "\n",
    "    val_loss, val_acc, val_labels_idx, val_preds_idx, val_probs, val_labels_mh = evaluate(\n",
    "        mert_model, head, processor, val_loader, criterion,\n",
    "        device, num_layers, hidden_dim, AMP_ENABLED, AMP_TORCH_DTYPE, CHUNK_BATCH_SIZE_EFFECTIVE)\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Track best\n",
    "    improved = val_acc > best_val_acc\n",
    "    if improved:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Save every epoch\n",
    "    ckpt_data = {\n",
    "        \"head_state_dict\": {k: v.cpu().clone() for k, v in head.state_dict().items()},\n",
    "        \"lora_state_dict\": {n: p.cpu().clone() for n, p in mert_model.named_parameters() if p.requires_grad} if USE_LORA else None,\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"class_map\": IDX_TO_METER,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"pooled_dim\": pooled_dim,\n",
    "        \"head_dim\": 256,\n",
    "        \"num_classes\": len(CLASS_METERS),\n",
    "        \"dropout\": HEAD_DROPOUT,\n",
    "        \"val_accuracy\": best_val_acc,\n",
    "        \"val_loss\": best_val_loss,\n",
    "        \"epoch\": epoch,\n",
    "        \"lora_rank\": LORA_RANK if USE_LORA else 0,\n",
    "        \"lora_alpha\": LORA_ALPHA if USE_LORA else 0,\n",
    "        \"model_type\": \"MERTFineTuned\",\n",
    "    }\n",
    "    torch.save(ckpt_data, CKPT_PATH)\n",
    "    if improved:\n",
    "        torch.save(ckpt_data, BEST_CKPT_PATH)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    marker = \" *\" if improved else \"\"\n",
    "    print(f\"{epoch:3d}  {train_loss:7.4f}  {train_acc:5.1%}  {val_loss:7.4f}  {val_acc:5.1%}  {current_lr:.1e}  {elapsed:4.0f}s{marker}\")\n",
    "    print(f\"      Val class: {format_per_class_watchdog(val_labels_idx, val_preds_idx, val_probs, val_labels_mh)}\")\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest val: {best_val_acc:.1%}, saved to {BEST_CKPT_PATH.name if BEST_CKPT_PATH.exists() else CKPT_PATH.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint (fallback to latest)\n",
    "EVAL_CKPT_PATH = BEST_CKPT_PATH if BEST_CKPT_PATH.exists() else CKPT_PATH\n",
    "if EVAL_CKPT_PATH.exists():\n",
    "    ckpt = torch.load(EVAL_CKPT_PATH, weights_only=False, map_location=device)\n",
    "    head.load_state_dict(ckpt[\"head_state_dict\"])\n",
    "    head = head.to(device)\n",
    "    if ckpt.get(\"lora_state_dict\") and USE_LORA:\n",
    "        for name, param_data in ckpt[\"lora_state_dict\"].items():\n",
    "            parts = name.split(\".\")\n",
    "            obj = mert_model\n",
    "            for part in parts[:-1]:\n",
    "                obj = getattr(obj, part)\n",
    "            getattr(obj, parts[-1]).data.copy_(param_data.to(device))\n",
    "    print(f\"Loaded: {EVAL_CKPT_PATH.name} (epoch {ckpt.get('epoch', '?')}, val {ckpt.get('val_accuracy', 0):.1%})\")\n",
    "\n",
    "test_loss, test_acc, test_labels, test_preds, test_probs, test_labels_mh = evaluate(\n",
    "    mert_model, head, processor, test_loader, criterion,\n",
    "    device, num_layers, hidden_dim, AMP_ENABLED, AMP_TORCH_DTYPE, CHUNK_BATCH_SIZE_EFFECTIVE)\n",
    "\n",
    "n_correct = sum(1 for a, b in zip(test_labels, test_preds) if a == b)\n",
    "print(f\"\\nTest: {n_correct}/{len(test_labels)} = {test_acc:.1%}\")\n",
    "print_eval_metrics(test_labels, test_preds, test_probs, test_labels_mh)\n",
    "\n",
    "# Save test acc to checkpoints\n",
    "if CKPT_PATH.exists():\n",
    "    latest_ckpt = torch.load(CKPT_PATH, weights_only=False, map_location=device)\n",
    "    latest_ckpt[\"test_accuracy\"] = test_acc\n",
    "    torch.save(latest_ckpt, CKPT_PATH)\n",
    "if BEST_CKPT_PATH.exists():\n",
    "    best_ckpt = torch.load(BEST_CKPT_PATH, weights_only=False, map_location=device)\n",
    "    best_ckpt[\"test_accuracy\"] = test_acc\n",
    "    torch.save(best_ckpt, BEST_CKPT_PATH)\n",
    "\n",
    "# Release GPU\n",
    "print(\"\\nReleasing GPU...\")\n",
    "from google.colab import runtime\n",
    "runtime.unassign()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
